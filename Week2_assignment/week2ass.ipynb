{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0K_2hhKIrK1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
        "df.columns = ['label', 'message']\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hx-ZJR8S96wk",
        "outputId": "bd4f5476-7697-425b-923b-d86450d00a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2b439b8da00a4697bfa79c6c6c65066d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1438482686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install contractions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             self._str = self._format_parsed_parts(self._drv, self._root,\n\u001b[0m\u001b[1;32m    543\u001b[0m                                                   self._parts) or '.'\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_format_parsed_parts\u001b[0;34m(cls, drv, root, parts)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_parsed_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdrv\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdrv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXHwoze5JUUg",
        "outputId": "9f2bf134-b7ba-401f-cb61-37729f4cba71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "  tokens = text.lower().split()\n",
        "  tokens = [ word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-1lQ5ufJ5N3",
        "outputId": "cfacf497-de5b-4040-b1dc-e595580b7863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      label                                            message  \\\n",
            "0         0  Go until jurong point, crazy.. Available only ...   \n",
            "1         0                      Ok lar... Joking wif u oni...   \n",
            "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "3         0  U dun say so early hor... U c already then say...   \n",
            "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
            "...     ...                                                ...   \n",
            "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
            "5568      0              Will Ì_ b going to esplanade fr home?   \n",
            "5569      0  Pity, * was in mood for that. So...any other s...   \n",
            "5570      0  The guy did some bitching but I acted like i'd...   \n",
            "5571      0                         Rofl. Its true to its name   \n",
            "\n",
            "                                                 tokens  \n",
            "0     [go, jurong, available, bugis, n, great, world...  \n",
            "1                                  [ok, joking, wif, u]  \n",
            "2     [free, entry, wkly, comp, win, fa, cup, final,...  \n",
            "3                   [u, dun, say, early, u, c, already]  \n",
            "4             [nah, think, goes, lives, around, though]  \n",
            "...                                                 ...  \n",
            "5567  [time, tried, contact, u, pound, claim, call, ...  \n",
            "5568                          [b, going, esplanade, fr]  \n",
            "5569                                             [mood]  \n",
            "5570  [guy, bitching, acted, like, interested, buyin...  \n",
            "5571                                       [true, name]  \n",
            "\n",
            "[5572 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "df['tokens'] = df['message'].apply(preprocess_text)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bxedg7MJ_1h"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "w2v_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def vectorize(tokens):\n",
        "  valid_tokens = [token for token in tokens if token in w2v_model]\n",
        "  if not valid_tokens:\n",
        "    return np.zeros(w2v_model.vector_size)\n",
        "  vectors = [w2v_model[token] for token in valid_tokens]\n",
        "  return np.mean(vectors, axis=0)\n",
        "\n",
        "df['vector'] = df['tokens'].apply(vectorize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-2zPuwTKpm8"
      },
      "outputs": [],
      "source": [
        "X = np.vstack(df['vector'].values)\n",
        "y = df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQMUV9uKK6a1"
      },
      "outputs": [],
      "source": [
        "logModel = LogisticRegression(max_iter=1000)\n",
        "logModel.fit(X_train, y_train)\n",
        "y_pred = logModel.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzIZwaLbK_9x",
        "outputId": "25da2b85-8712-4a00-e5f2-45de0a17bd46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message: Assignment pending due in 1 hour\n",
            "Prediction: ham\n"
          ]
        }
      ],
      "source": [
        "def predict_message_class(model, w2v_model, message):\n",
        "    tokens = preprocess_text(message)\n",
        "    vector = vectorize(tokens).reshape(1, -1)\n",
        "    prediction = model.predict(vector)[0]\n",
        "    return \"spam\" if prediction == 1 else \"ham\"\n",
        "\n",
        "msg = \"Assignment pending due in 1 hour\"\n",
        "print(f\"Message: {msg}\")\n",
        "print(\"Prediction:\", predict_message_class(logModel, w2v_model, msg))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3uItj4MLMXU",
        "outputId": "a28f501c-0cd8-451e-a758-1593ce60ab29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message: Win 10Rs in 1 min\n",
            "Prediction: spam\n"
          ]
        }
      ],
      "source": [
        "msg1 = 'Win 10Rs in 1 min'\n",
        "print(f\"Message: {msg1}\")\n",
        "print(\"Prediction:\", predict_message_class(logModel, w2v_model, msg1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo0KUk1BLVly"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAEwaotq9VmF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le5lF2pP9Vpc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zPGIzO_9Wyb"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zaBM19Xx9X5J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "data = pd.read_csv('Tweets.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DowS1Qde-N3c",
        "outputId": "b6c7c464-b46b-43fe-8181-ba93a313a7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
            "0      570306133677760513           neutral                        1.0000   \n",
            "1      570301130888122368          positive                        0.3486   \n",
            "2      570301083672813571           neutral                        0.6837   \n",
            "3      570301031407624196          negative                        1.0000   \n",
            "4      570300817074462722          negative                        1.0000   \n",
            "...                   ...               ...                           ...   \n",
            "14635  569587686496825344          positive                        0.3487   \n",
            "14636  569587371693355008          negative                        1.0000   \n",
            "14637  569587242672398336           neutral                        1.0000   \n",
            "14638  569587188687634433          negative                        1.0000   \n",
            "14639  569587140490866689           neutral                        0.6771   \n",
            "\n",
            "               negativereason  negativereason_confidence         airline  \\\n",
            "0                         NaN                        NaN  Virgin America   \n",
            "1                         NaN                     0.0000  Virgin America   \n",
            "2                         NaN                        NaN  Virgin America   \n",
            "3                  Bad Flight                     0.7033  Virgin America   \n",
            "4                  Can't Tell                     1.0000  Virgin America   \n",
            "...                       ...                        ...             ...   \n",
            "14635                     NaN                     0.0000        American   \n",
            "14636  Customer Service Issue                     1.0000        American   \n",
            "14637                     NaN                        NaN        American   \n",
            "14638  Customer Service Issue                     0.6659        American   \n",
            "14639                     NaN                     0.0000        American   \n",
            "\n",
            "      airline_sentiment_gold             name negativereason_gold  \\\n",
            "0                        NaN          cairdin                 NaN   \n",
            "1                        NaN         jnardino                 NaN   \n",
            "2                        NaN       yvonnalynn                 NaN   \n",
            "3                        NaN         jnardino                 NaN   \n",
            "4                        NaN         jnardino                 NaN   \n",
            "...                      ...              ...                 ...   \n",
            "14635                    NaN  KristenReenders                 NaN   \n",
            "14636                    NaN         itsropes                 NaN   \n",
            "14637                    NaN         sanyabun                 NaN   \n",
            "14638                    NaN       SraJackson                 NaN   \n",
            "14639                    NaN        daviddtwu                 NaN   \n",
            "\n",
            "       retweet_count                                               text  \\\n",
            "0                  0                @VirginAmerica What @dhepburn said.   \n",
            "1                  0  @VirginAmerica plus you've added commercials t...   \n",
            "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
            "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
            "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
            "...              ...                                                ...   \n",
            "14635              0  @AmericanAir thank you we got on a different f...   \n",
            "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
            "14637              0  @AmericanAir Please bring American Airlines to...   \n",
            "14638              0  @AmericanAir you have my money, you change my ...   \n",
            "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
            "\n",
            "      tweet_coord              tweet_created tweet_location  \\\n",
            "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
            "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
            "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
            "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
            "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
            "...           ...                        ...            ...   \n",
            "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
            "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
            "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
            "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
            "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
            "\n",
            "                    user_timezone  \n",
            "0      Eastern Time (US & Canada)  \n",
            "1      Pacific Time (US & Canada)  \n",
            "2      Central Time (US & Canada)  \n",
            "3      Pacific Time (US & Canada)  \n",
            "4      Pacific Time (US & Canada)  \n",
            "...                           ...  \n",
            "14635                         NaN  \n",
            "14636                         NaN  \n",
            "14637                         NaN  \n",
            "14638  Eastern Time (US & Canada)  \n",
            "14639                         NaN  \n",
            "\n",
            "[14640 rows x 15 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tz30N3w-1Mb",
        "outputId": "84511496-9f65-4498-e241-32412acfe120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fid9OR2k-PwJ"
      },
      "outputs": [],
      "source": [
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB, 'R': wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  tweet=contractions.fix(tweet)\n",
        "\n",
        "\n",
        "  tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)  # URLs\n",
        "  tweet = re.sub(r'@\\w+', '', tweet)                    # Mentions\n",
        "  tweet = re.sub(r'#\\w+', '', tweet)                    # Hashtags\n",
        "  tweet = re.sub(r'[^\\w\\s]', '', tweet)                 # Punctuation\n",
        "\n",
        "  tokens = tweet.split()\n",
        "\n",
        "  lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
        "  return lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ldMCjXnQ_sXT"
      },
      "outputs": [],
      "source": [
        "# Preproseesing tweets\n",
        "data['tokens'] = data['text'].apply(preprocess_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTslcgzAdgt",
        "outputId": "4046285b-a72e-4e70-c82c-1f07709a9efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
            "0      570306133677760513           neutral                        1.0000   \n",
            "1      570301130888122368          positive                        0.3486   \n",
            "2      570301083672813571           neutral                        0.6837   \n",
            "3      570301031407624196          negative                        1.0000   \n",
            "4      570300817074462722          negative                        1.0000   \n",
            "...                   ...               ...                           ...   \n",
            "14635  569587686496825344          positive                        0.3487   \n",
            "14636  569587371693355008          negative                        1.0000   \n",
            "14637  569587242672398336           neutral                        1.0000   \n",
            "14638  569587188687634433          negative                        1.0000   \n",
            "14639  569587140490866689           neutral                        0.6771   \n",
            "\n",
            "               negativereason  negativereason_confidence         airline  \\\n",
            "0                         NaN                        NaN  Virgin America   \n",
            "1                         NaN                     0.0000  Virgin America   \n",
            "2                         NaN                        NaN  Virgin America   \n",
            "3                  Bad Flight                     0.7033  Virgin America   \n",
            "4                  Can't Tell                     1.0000  Virgin America   \n",
            "...                       ...                        ...             ...   \n",
            "14635                     NaN                     0.0000        American   \n",
            "14636  Customer Service Issue                     1.0000        American   \n",
            "14637                     NaN                        NaN        American   \n",
            "14638  Customer Service Issue                     0.6659        American   \n",
            "14639                     NaN                     0.0000        American   \n",
            "\n",
            "      airline_sentiment_gold             name negativereason_gold  \\\n",
            "0                        NaN          cairdin                 NaN   \n",
            "1                        NaN         jnardino                 NaN   \n",
            "2                        NaN       yvonnalynn                 NaN   \n",
            "3                        NaN         jnardino                 NaN   \n",
            "4                        NaN         jnardino                 NaN   \n",
            "...                      ...              ...                 ...   \n",
            "14635                    NaN  KristenReenders                 NaN   \n",
            "14636                    NaN         itsropes                 NaN   \n",
            "14637                    NaN         sanyabun                 NaN   \n",
            "14638                    NaN       SraJackson                 NaN   \n",
            "14639                    NaN        daviddtwu                 NaN   \n",
            "\n",
            "       retweet_count                                               text  \\\n",
            "0                  0                @VirginAmerica What @dhepburn said.   \n",
            "1                  0  @VirginAmerica plus you've added commercials t...   \n",
            "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
            "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
            "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
            "...              ...                                                ...   \n",
            "14635              0  @AmericanAir thank you we got on a different f...   \n",
            "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
            "14637              0  @AmericanAir Please bring American Airlines to...   \n",
            "14638              0  @AmericanAir you have my money, you change my ...   \n",
            "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
            "\n",
            "      tweet_coord              tweet_created tweet_location  \\\n",
            "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
            "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
            "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
            "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
            "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
            "...           ...                        ...            ...   \n",
            "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
            "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
            "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
            "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
            "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
            "\n",
            "                    user_timezone  \\\n",
            "0      Eastern Time (US & Canada)   \n",
            "1      Pacific Time (US & Canada)   \n",
            "2      Central Time (US & Canada)   \n",
            "3      Pacific Time (US & Canada)   \n",
            "4      Pacific Time (US & Canada)   \n",
            "...                           ...   \n",
            "14635                         NaN   \n",
            "14636                         NaN   \n",
            "14637                         NaN   \n",
            "14638  Eastern Time (US & Canada)   \n",
            "14639                         NaN   \n",
            "\n",
            "                                                  tokens  \n",
            "0                                            [what, say]  \n",
            "1      [plus, you, have, add, commercial, to, the, ex...  \n",
            "2      [i, do, not, today, must, mean, i, need, to, t...  \n",
            "3      [it, be, really, aggressive, to, blast, obnoxi...  \n",
            "4      [and, it, be, a, really, big, bad, thing, abou...  \n",
            "...                                                  ...  \n",
            "14635  [thank, you, we, get, on, a, different, flight...  \n",
            "14636  [leave, over, 20, minute, late, flight, no, wa...  \n",
            "14637             [please, bring, american, airline, to]  \n",
            "14638  [you, have, my, money, you, change, my, flight...  \n",
            "14639  [we, have, 8, people, so, we, need, 2, know, h...  \n",
            "\n",
            "[14640 rows x 16 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV6_rLxmBGOJ",
        "outputId": "8dd487b3-32f2-47b9-fc2d-ae4a713de7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as ge\n",
        "w2v_model = ge.load(\"glove-wiki-gigaword-100\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def vectorize(tokens):\n",
        "  valid_tokens = [token for token in tokens if token in w2v_model]\n",
        "  if not valid_tokens:\n",
        "    return np.zeros(w2v_model.vector_size)\n",
        "  vectors = [w2v_model[token] for token in valid_tokens]\n",
        "  return np.mean(vectors, axis=0)\n",
        "\n",
        "data['vector'] = data['tokens'].apply(vectorize)"
      ],
      "metadata": {
        "id": "ZeXZMOWeS5V8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QawfqAlXBb3i",
        "outputId": "8491d2d3-3da7-4099-9702-f6ed6a08233c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 ... 0 2 0]\n"
          ]
        }
      ],
      "source": [
        "X = np.vstack(data['vector'].values)\n",
        "y = data['airline_sentiment'].values\n",
        "y = np.where(y == 'neutral', 0, y)\n",
        "y = np.where(y == 'positive', 1, y)\n",
        "y = np.where(y == 'negative', 2, y)\n",
        "y = y.astype(int) # Convert to integer type\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PPssk-GLCYPc"
      },
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(max_iter=2000)\n",
        "logReg.fit(X_train, y_train)\n",
        "y_pred = logReg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvHLTgEdTD9b",
        "outputId": "10a01b63-8d9a-4758-8aff-2944cc80ae80"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7540983606557377\n",
            "Recall: 0.7540983606557377\n",
            "Precision: 0.7400951674698802\n",
            "F1 Score: 0.7303688825291269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet_senti(model, w2v_model, tweet):\n",
        "    tokens = preprocess_tweet(tweet)\n",
        "    vector = vectorize(tokens).reshape(1, -1)\n",
        "    prediction = model.predict(vector)[0]\n",
        "    return \"neutral\" if prediction == 0 else \"positive\" if prediction == 1 else \"negative\""
      ],
      "metadata": {
        "id": "VxqgUDz7TM5z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt1 = 'Uncomfortable experience, bad service'\n",
        "print(f\"Tweet: {twt1}\")\n",
        "print(\"Prediction:\", predict_tweet_senti(logReg, w2v_model, twt1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSr-yeZEUtgg",
        "outputId": "d8291572-82d9-4373-e8d7-c7ab787aa178"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: Uncomfortable experience, bad service\n",
            "Prediction: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twt2= 'Good Price for the experience'\n",
        "print(f\"Tweet: {twt2}\")\n",
        "print(\"Prediction:\", predict_tweet_senti(logReg, w2v_model, twt2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTp2zLKoUyDA",
        "outputId": "5f17e292-4d6e-45ed-87eb-964801d582f6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: Good Price for the experience\n",
            "Prediction: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twt3 = '@Indigo My Name is Ayush'\n",
        "print(f\"Tweet: {twt3}\")\n",
        "print(\"Prediction:\", predict_tweet_senti(logReg, w2v_model, twt3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOdlHOn3U1YW",
        "outputId": "b208b956-4bc3-4ae9-ddff-b52c01aed743"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: @Indigo My Name is Ayush\n",
            "Prediction: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atZtDXfxVP0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}